{"cells":[{"cell_type":"markdown","metadata":{"id":"Jf-_uA8P5Hf7"},"source":["Assignment 3:\n","\n","Name:\n","\n","UID:\n","\n","Please submit to ELMS\n","- a PDF containing all outputs (by executing **Run all**)\n","- your ipynb notebook containing all the code\n","\n","I understand the policy on academic integraty (collaboration and the use of online material).\n","Please sign your name here:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hy_Iqu-qnIj3"},"outputs":[],"source":["## Import necessary libraries here (You can add libraries you want to use here)\n","import cv2\n","import numpy as np\n","from scipy.io import loadmat\n","from scipy import ndimage\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"UurFy8hB5BeP"},"source":["# Part 1: Shape Alignment (30 Points)"]},{"cell_type":"markdown","metadata":{"id":"baagTPK5aa9r"},"source":["## Overview\n","In this problem, you will write a function that aligns two sets of points using\n","iterative closest point (ICP) to output an affine image transformation $T$  where $T$ is a transformation that maps non-zero points in $im1$ to non-zero points in $im2$.\n","\n","<img src=\"https://www.cs.umd.edu/class/spring2023/cmsc426-0201/hw_images/shape_align.jpeg\" width=\"1000\"/>\n","\n","We have provided 25 image pairs provided to you to test your code, but you don't have to try all of them. Please include the output of your code on the 10 image pairs from the examples provided in the supplementary material. We have included functions\n","**evalAlignmentAll and displayAlignment to help with evaluation and display**.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qWCfxrUA7LNb"},"source":["## Data\n","\n","**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5aF6Biq7LvQ"},"outputs":[],"source":["# Download Data -- run this cell only one time per runtime\n","!gdown 18Px9uQyY1fGGyEAQhzt3h4yDQonU_Sgm\n","!unzip \"/content/part2_images.zip\" -d \"/content/\""]},{"cell_type":"markdown","metadata":{"id":"Nqgb2YNJfQtV"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itHJzXsKfSpQ"},"outputs":[],"source":["def evalAlignment(aligned1, im2):\n","  '''\n","  Computes the error of the aligned image (aligned1) and im2, as the\n","  average of the average minimum distance of a point in aligned1 to a point in im2\n","  and the average minimum distance of a point in im2 to aligned1.\n","  '''\n","  d2 = ndimage.distance_transform_edt(1-im2) #distance transform\n","  err1 = np.mean(np.mean(d2[aligned1 > 0]))\n","  d1 = ndimage.distance_transform_edt(1-aligned1);\n","  err2 = np.mean(np.mean(d2[im2 > 0]))\n","  err = (err1+err2)/2;\n","  return err\n","\n","def displayAlignment(im1, im2, aligned1, thick=False):\n","  '''\n","  Displays the alignment of im1 to im2\n","     im1: first input image to alignment algorithm (im1(y, x)=1 if (y, x)\n","      is an original point in the first image)\n","     im2: second input image to alignment algorithm\n","     aligned1: new1(y, x) = 1 iff (y, x) is a rounded transformed point from the first time\n","     thick: true if a line should be thickened for display\n","  '''\n","  if thick:\n","    # for thick lines (looks better for final display)\n","    dispim = np.concatenate((cv2.dilate(im1.astype('uint8'), np.ones((3,3), np.uint8), iterations=1), \\\n","                             cv2.dilate(aligned1.astype('uint8'), np.ones((3,3), np.uint8), iterations=1), \\\n","                             cv2.dilate(im2.astype('uint8'), np.ones((3,3), np.uint8), iterations=1)), axis=-1)\n","  else:\n","    # for thin lines (faster)\n","    dispim = np.concatenate((im1, aligned1, im2), axis = -1)\n","  return dispim\n"]},{"cell_type":"markdown","metadata":{"id":"cUgcCzJRAQJ1"},"source":["## Code (15 pts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywdGq5E55thm"},"outputs":[],"source":["def align_shape(im1, im2):\n","  '''\n","  im1: input edge image 1\n","  im2: input edge image 2\n","\n","  Output: transformation T [3] x [3]\n","  '''\n","  # YOUR CODE HERE\n","  return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vCuTHeShm41"},"outputs":[],"source":["imgPath = '/content/part2_images/';\n","\n","objList = ['apple', 'bat', 'bell', 'bird', 'Bone', 'bottle', 'brick', \\\n","    'butterfly', 'camel', 'car', 'carriage', 'cattle', 'cellular_phone', \\\n","    'chicken', 'children', 'device7', 'dog', 'elephant', 'face', 'fork', 'hammer', \\\n","    'Heart', 'horse', 'jar', 'turtle']\n","\n","numObj = len(objList)\n","\n","# Add code to run align_shape() and display the results and the errors\n","for i in range(numObj):\n","    image_1 = ?\n","    image_2 = ?\n","    \n","# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"8AmYt5Vu9BGq"},"source":["## Write-up (15 pt)\n","\n","1. (5 pts) Give a brief explanation of your algorithm, initialization, and model of the transformation.\n","\n","2. (10 pts) For each result, give:\n","  1.   The alignment display\n","  2.   The final error\n","  3.   The runtime\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lRUDQ9Xk5e3v"},"source":["**Include your write-up here**"]},{"cell_type":"markdown","metadata":{"id":"F6UrONbD4LcH"},"source":["# Part 2: Object Instance Recognition (20 points)"]},{"cell_type":"markdown","metadata":{"id":"EGoM9Ra_4QLS"},"source":["## Overview\n","This problem explores the Lowe-style object instance recognition.\n","\n","Implement the nearest neighbor distance ratio test using the pre-computed SIFT features SIFT_features.mat provided in the supplementary material. The Frame1, Frame2 indicate the 2D position, scales, and the orientation of the descriptors and Descriptor1, Descriptor2 are the correspondin 128-D SIFT features. Display the matches like this:\n","\n","<img src=\"https://www.cs.umd.edu/class/spring2023/cmsc426-0201/hw_images/Lowe_rec.jpeg\" width=\"1000\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZDy-8Y7Oj7kJ"},"source":["## Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0AUhSYpj8ed"},"outputs":[],"source":["# Download Data -- run this cell only one time per runtime\n","!gdown 10ByzpFbB-z178VGjwmCwc95wInD8vpNM # SIFT Features\n","!gdown 1KLWGMtDEMNNrmzd3Qezrs2-NQR52OfoU # Stop sign image 1\n","!gdown 13y-o1vdGN6CqqPuUcgU7pIxODTxrYS7J # Stop sign image 1"]},{"cell_type":"markdown","metadata":{"id":"gQW8iyjH_Ijd"},"source":["## Code (10 pts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5I3eiXvz_IGj"},"outputs":[],"source":["img1 = cv2.imread('/content/stop1.jpg')\n","img2 = cv2.imread('/content/stop2.jpg')\n","\n","## inside the sift are:\n","## Descriptor1, Descriptor2: SIFT features from image 1 and image 2\n","## Frame1, Frame2: position, scale, rotation of keypoints\n","data = loadmat('/content/SIFT_features.mat')\n","Frame1 = data['Frame1']\n","Descriptor1 = data['Descriptor1']\n","Frame2 = data['Frame2']\n","Descriptor2 = data['Descriptor2']\n","\n","# YOUR CODE HERE\n","\n","## matches: a 2 x N array of indices that indicates which keypoints from image\n","## 1 match which points in image 2\n","\n","## Display the matched keypoints\n","# YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"D7ApiN9gAP6Z"},"source":["## Write-up (10 pts)\n","\n","(5 pts) Display:\n","\n","1. the matches by thresholding nearest neighbor distances.\n","\n","2. the matches by thresholding the distance ratio.\n","\n","(5 pts) Describe the differences of (1) and (2)."]},{"cell_type":"markdown","metadata":{"id":"fEa_-lFkux7C"},"source":["**Your answer here**"]}],"metadata":{"colab":{"provenance":[{"file_id":"1RHU9GoTofyAK_5Uzas9fUwNOAs9sfBn2","timestamp":1707076545395},{"file_id":"1v_tjVBmOPoqHeEMzejEPjZeqCoj-NAO7","timestamp":1678335854028},{"file_id":"1BktGJlr0DZU3y2Jd4yaTq6ZK14FFQEA0","timestamp":1663906646975}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
