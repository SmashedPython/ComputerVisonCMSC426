{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 4:\n",
        "\n",
        "Name:\n",
        "\n",
        "UID:\n",
        "\n",
        "Please submit to ELMS\n",
        "- a PDF containing all outputs (by executing **Run all**)\n",
        "- your ipynb notebook containing all the code\n",
        "\n",
        "I understand the policy on academic integraty (collaboration and the use of online material).\n",
        "Please sign your name here:"
      ],
      "metadata": {
        "id": "flVP-gDsfegx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import necessary libraries here (You can add libraries you want to use here)\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Cee3vAcLrOs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Epipolar Geometry (30 Points)\n"
      ],
      "metadata": {
        "id": "f3j4FysdrPpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this problem, you will implement an algorithm for automatically estimating homography with RANSAC. In the file matches.mat, we provide the detected Harris corners row-column positions in variables r1 c1 for the first image; variables r2 c2 for the second image; and the corresponding matched pairs in the variable matches.\n",
        "\n",
        "<!-- <img src=\"https://drive.google.com/uc?id=1Tr723u5OXmwkd4RDmu9z886ITJU9j1cL&export=download\" width=\"800\"/> -->\n",
        "\n",
        "<img src=\"https://www.cs.umd.edu/class/spring2023/cmsc426-0201/hw_images/epipolar_geometry.jpg\" width=\"800\"/>\n",
        "\n",
        "\n",
        "The outline of the normalized 8-point algorithm:\n",
        "\n",
        "<img src=\"https://www.cs.umd.edu/class/spring2023/cmsc426-0201/hw_images/8pointalgo.jpg\" width=\"700\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "-PQh0Alx6ibx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ],
      "metadata": {
        "id": "WhMdiMNjB9R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1cn3_SscjlLrf4BzUWe8MV-XqMqBY4Nj_\n",
        "# Or download from https://drive.google.com/file/d/1cn3_SscjlLrf4BzUWe8MV-XqMqBY4Nj_/view and upload\n",
        "\n",
        "!unzip \"/content/Part1_data.zip\" -d \"/content/\"\n",
        "# Load Matches\n",
        "data = loadmat('/content/Part1_data/matches.mat')\n",
        "r1 = data['r1']\n",
        "r2 = data['r2']\n",
        "c1 = data['c1']\n",
        "c2 = data['c2']\n",
        "matches = data['matches']"
      ],
      "metadata": {
        "id": "3LNhf2S4CAIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf1f434-7032-4c99-cbac-9b29cd1bb33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cn3_SscjlLrf4BzUWe8MV-XqMqBY4Nj_\n",
            "To: /content/Part1_data.zip\n",
            "\r  0% 0.00/157k [00:00<?, ?B/s]\r100% 157k/157k [00:00<00:00, 68.8MB/s]\n",
            "Archive:  /content/Part1_data.zip\n",
            "   creating: /content/Part1_data/\n",
            "  inflating: /content/Part1_data/chapel00.png  \n",
            "  inflating: /content/Part1_data/chapel01.png  \n",
            "  inflating: /content/Part1_data/matches.mat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Keypoints\n",
        "x1 = c1[matches[:,0]-1]\n",
        "y1 = r1[matches[:,0]-1]\n",
        "x2 = c2[matches[:,1]-1]\n",
        "y2 = r2[matches[:,1]-1]"
      ],
      "metadata": {
        "id": "7JTA_rZ4y_V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "QyGOs95Bslvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import gaussian_filter as gf\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def show_image(img, scale=1.0):\n",
        "    plt.figure(figsize=scale* plt.figaspect(1))\n",
        "    plt.imshow(img, interpolation='nearest')\n",
        "    plt.gray()\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UkmZo6KWslNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code (15 pt)"
      ],
      "metadata": {
        "id": "JjIGNyapNU_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint\n",
        "\n",
        "*   For visualization, you can use cv2.line, cv2.circle or any other helper functions in opencv or matplotlib.\n"
      ],
      "metadata": {
        "id": "5JB8fYJCBI6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x, y):\n",
        "  \"\"\"\n",
        "  Function: find the transformation T to make coordinates zero mean and the variance as sqrt(2)\n",
        "  Input: x, y - coordinates\n",
        "  Output: normalized coordinates, transformation T\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE:\n",
        "\n",
        "  return None\n",
        "\n",
        "def ransacF(x1, y1, x2, y2, num_iterations=1000, threshold=0.01):\n",
        "  \"\"\"\n",
        "  Find normalization matrix\n",
        "  Transform point set 1 and 2\n",
        "  RANSAC based 8-point algorithm\n",
        "  Input:\n",
        "     x1, y1, x2, y2 - coordinates\n",
        "     num_iterations - how many iterations\n",
        "     threshold - threshold for inlier check\n",
        "  Output:\n",
        "     Best fundamental matrix\n",
        "     corresponding inlier indices\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE:\n",
        "\n",
        "  # Hint:\n",
        "  # for ... in num_iterations:\n",
        "  #    1. Randomly select 8 points\n",
        "\n",
        "  #    2. Call computeF()\n",
        "\n",
        "  #    3. Call getInliers()\n",
        "\n",
        "  #    4. Update F and inliers.\n",
        "\n",
        "  return None\n",
        "\n",
        "def computeF(x1, y1, x2, y2):\n",
        "  \"\"\"\n",
        "  Function: compute fundamental matrix from corresponding points\n",
        "  Input:\n",
        "     x1, y1, x2, y2 - coordinates\n",
        "  Output:\n",
        "     fundamental matrix, 3x3\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE:\n",
        "\n",
        "  # Hint:\n",
        "  # 1. Make matrix A\n",
        "\n",
        "  # 2. Do SVD for A\n",
        "\n",
        "  # 3. Find fundamental matrix F\n",
        "\n",
        "  return None\n",
        "\n",
        "def getInliers(x1, y1, x2, y2, F, thresh):\n",
        "  \"\"\"\n",
        "   Function: implement the criteria checking inliers.\n",
        "   Input:\n",
        "     x1, y1, x2, y2 - coordinates\n",
        "     F - estimated fundamental matrix, 3x3\n",
        "     thresh - threshold for passing the error\n",
        "   Output:\n",
        "     inlier indices\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE:\n",
        "\n",
        "  # Hints:\n",
        "  # Say we project the points onto the second image\n",
        "  # 1. Compute epipolar lines. Here a line is expressed as a vector\n",
        "  # Recall line equation: ax + by + c = 0\n",
        "\n",
        "  # 2. Calculate the distances of points in the second image from the corresponding lines\n",
        "  # Recall that the distance between a point and a line is\n",
        "  # d = |ax + by + c| / sqrt(a^2 + b^2)\n",
        "\n",
        "  # 3. Check distances with the threshold and find inliers\n",
        "\n",
        "  return None\n"
      ],
      "metadata": {
        "id": "lS5zihq7NRjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No rush, let's do a sanity check for computeF()\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# A synthetic dataset with matched points\n",
        "p1 = np.array([[83, 245], [39, 461], [284, 243],[328, 459],   # points of image1 in 2D coordinates\n",
        "              [331, 166], [286, 47], [291, 332],[296, 303]])\n",
        "p2 = np.array([[36, 155], [34, 625], [329, 158],[323, 627],   # points of image2 in 2D coordinates\n",
        "              [375, 714], [375, 67], [313, 398],[327, 331]])\n",
        "\n",
        "px1, py1 = p1[:, :1], p1[:, 1:2]\n",
        "px2, py2 = p2[:, :1], p2[:, 1:2]\n",
        "\n",
        "# Normalize points\n",
        "norm_p1, T1 = normalize(px1, py1)\n",
        "norm_x1, norm_y1 = norm_p1[:, :1], norm_p1[:, 1:2]\n",
        "norm_p2, T2 = normalize(px2, py2)\n",
        "norm_x2, norm_y2 = norm_p2[:, :1], norm_p2[:, 1:2]\n",
        "\n",
        "# Compute fundamental matrix with computeF()\n",
        "F_ours = computeF(norm_x1, norm_y1, norm_x2, norm_y2)\n",
        "F_ours = np.matmul(np.matmul(np.transpose(T2),F_ours), T1)\n",
        "F_ours = F_ours / F_ours[2, 2]\n",
        "print(\"Our F: \", F_ours)\n",
        "# Compute fundamental matrix with cv2.findFundamentalMat()\n",
        "F_cv2, _ = cv2.findFundamentalMat(p1, p2, method=cv2.FM_8POINT)\n",
        "print(\"CV2 F: \", F_cv2)\n",
        "\n",
        "# Display difference, expected to be very small.\n",
        "mse_err = mean_squared_error(F_ours, F_cv2)\n",
        "print(f'\\n MSE of fundamental matrixes = {mse_err }')\n",
        "\n",
        "if mse_err < 1e-12:\n",
        "  print(\"Sanity check passed!\")\n",
        "else:\n",
        "  print(\"Sanify check failed!\")"
      ],
      "metadata": {
        "id": "Tv7xIzVBOQkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the functions implemented above, write the CODE HERE to get the write up\n",
        "# estimate the fundamental matrix F\n",
        "\n",
        "\n",
        "# 1. Normalize coordinates. Call `normalize()` above.\n",
        "\n",
        "\n",
        "# 2. Run 8-point algorithm with RANSAC to estimate F. Run `ransacF()`.\n",
        "# You can decide num_iterations by computing\n",
        "# log(1 - p)/log(1 - (1 - e)**s), check Lecture 10.\n",
        "\n",
        "\n",
        "# 3. De-normalize F using T, T' from 1.\n",
        "\n",
        "\n",
        "# 4. Normalize F so that the last entry is 1.\n",
        "# NOTE: this normalization is different from what the normalize() function above does\n",
        "# this is just a simple normalization that scales the matrix so the last entry is 1\n",
        "\n",
        "\n",
        "# 5. Report normalized F from 4.\n"
      ],
      "metadata": {
        "id": "bwmxKGgMeewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write-up (15 pt)\n",
        "*   Describe what test you used for deciding inlier vs. outlier.\n",
        "*   Print the estimated fundamental matrix F after normalizing to unit length\n",
        "*   Randomly select 7 sets of matching points. Plot the corresponding epipolar lines and the points on each image. Show the two images (with plotted points and lines) next to each other.\n",
        "\n",
        "<!-- *   Plot the outlier keypoints with green dots on top of the first image -->\n",
        "<!-- *   Randomly select 7 sets of matching points. Plot the corresponding epipolar lines ('g’) and the points (with 'r+’) on each image. Show the two images (with plotted points and lines) next to each other. -->\n",
        "\n"
      ],
      "metadata": {
        "id": "jUvFYC17Bi5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the images with the corresponding epipolar lines\n",
        "# you may use any plotting library or tool you like\n",
        "\n",
        "# A possible way:\n",
        "# 1. Randomly select 7 pairs of points from two images. You can use `np.random.choice()`.\n",
        "\n",
        "# 2. Compute epipolar lines for selected points.\n",
        "# Recall how we represent a line above\n",
        "\n",
        "# 3. Draw lines and points for selected points\n",
        "# See example in Overview above.\n",
        "# You can use cv2.line() and cv2.circle().\n",
        "\n",
        "# 4. Show two images side by side.\n",
        "# You can use `np.concatenate()`.\n"
      ],
      "metadata": {
        "id": "Vuvmx2cLsV2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Image stitching (30 points)"
      ],
      "metadata": {
        "id": "QE_F4Hf5jUif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.cs.umd.edu/class/spring2023/cmsc426-0201/hw_images/image_stitching.png\" width=\"800\"/>\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this problem, you will implement an algorithm for automatically estimating the fundamental matrix F using RANSAC and the normalized 8-point algorithm.\n",
        "\n",
        "Image Stitching Algorithm Overview\n",
        "1. Detect keypoints\n",
        "2. Match keypoints\n",
        "3. Estimate homography with matched keypoints (using RANSAC)\n",
        "4. Combine images\n",
        "\n",
        "**Note:**  Do not use existing image stitching code, such as found on the web, and OpenCV."
      ],
      "metadata": {
        "id": "qSpCKlC_jcde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ],
      "metadata": {
        "id": "_EOoxrZurmYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1fnD0hJ8-_Rngsc-m96ghKtdZAMf0VTjy\n",
        "!unzip \"/content/hill.zip\" -d \"/content/hill\"\n",
        "\n",
        "!gdown 1v2BFVMV0McuD5BstLvDmo1U9MrFAByS5\n",
        "!unzip \"/content/tv.zip\" -d \"/content/tv\"\n"
      ],
      "metadata": {
        "id": "Tspp6CyMroUC",
        "outputId": "8cfe9696-8bf2-4b01-d5da-12c4ee6f1b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fnD0hJ8-_Rngsc-m96ghKtdZAMf0VTjy\n",
            "To: /content/hill.zip\n",
            "\r  0% 0.00/205k [00:00<?, ?B/s]\r100% 205k/205k [00:00<00:00, 6.39MB/s]\n",
            "Archive:  /content/hill.zip\n",
            "  inflating: /content/hill/1.JPG     \n",
            "  inflating: /content/hill/2.JPG     \n",
            "  inflating: /content/hill/3.JPG     \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v2BFVMV0McuD5BstLvDmo1U9MrFAByS5\n",
            "To: /content/tv.zip\n",
            "100% 130k/130k [00:00<00:00, 14.8MB/s]\n",
            "Archive:  /content/tv.zip\n",
            "  inflating: /content/tv/1.jpg       \n",
            "  inflating: /content/tv/2.jpg       \n",
            "  inflating: /content/tv/3.jpg       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint\n",
        "\n",
        "\n",
        "*   Use SIFT to find keypoint. You can use sift.detectAndCompute in opencv.\n",
        "*   For image warping and blending, you should first deterimne canvas size. You can use cv2.warpPerspective in opencv.\n",
        "\n",
        "An example is as follows:\n",
        "\n",
        "<img src=\"https://www.cs.umd.edu/class/spring2024/cmsc426-0101/img/stitch.jpg\" width=\"800\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EGGINt3MCKK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "YlVZP1tvMAxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def drawMatches(image1, kp1, image2, kp2, idx_pairs):\n",
        "    keypt1 = [cv2.KeyPoint(coord[1], coord[0], 40) for coord in kp1.tolist()]\n",
        "    keypt2 = [cv2.KeyPoint(coord[1], coord[0], 40) for coord in kp2.tolist()]\n",
        "    matches = [cv2.DMatch(pair[0], pair[1], 0) for pair in idx_pairs.tolist()]\n",
        "    return cv2.drawMatches(image1, keypt1, image2, keypt2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "def plot_matches(images, feature_coord, matches, img_idx):\n",
        "    matched_img = drawMatches(images[img_idx], feature_coord[img_idx], images[img_idx-1],\n",
        "                              feature_coord[img_idx-1], matches[img_idx-1])\n",
        "\n",
        "    cv2.imshow('Matches Found', matched_img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def est_homography(src, dest):\n",
        "    N = src.shape[0]\n",
        "    if N != dest.shape[0]:\n",
        "        raise ValueError(\"src and diff should have the same dimension\")\n",
        "    src_h = np.hstack((src, np.ones((N, 1))))\n",
        "    A = np.array([np.block([[src_h[n], np.zeros(3), -dest[n, 0] * src_h[n]],\n",
        "                            [np.zeros(3), src_h[n], -dest[n, 1] * src_h[n]]])\n",
        "                  for n in range(N)]).reshape(2 * N, 9)\n",
        "    [_, _, V] = np.linalg.svd(A)\n",
        "    return V.T[:, 8].reshape(3, 3)\n",
        "\n",
        "def apply_homography(H, src):\n",
        "    src_h = np.hstack((src, np.ones((src.shape[0], 1))))\n",
        "    dest =  src_h @ H.T\n",
        "    return (dest / dest[:,[2]])[:,0:2]"
      ],
      "metadata": {
        "id": "y9gRD27_MCDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code (15 pt)"
      ],
      "metadata": {
        "id": "YBhvHlseN_6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE:\n",
        "def crop_dark(panorama):\n",
        "    gray_panorama = cv2.cvtColor(panorama, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh = cv2.threshold(gray_panorama, 1, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    max_area_contour = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(max_area_contour)\n",
        "    panorama = panorama[y:y+h, x:x+w]\n",
        "\n",
        "    return panorama\n",
        "\n",
        "\n",
        "def find_homography_ransac(src, dest, threshold=0.5, max_iterations=1000):\n",
        "    \"\"\"\n",
        "    Run RANSAC to estimate H.\n",
        "    Input:\n",
        "    - src, dest: coordinates\n",
        "    - threshold: threshold for RANSAC\n",
        "    - max_iterations: number of iterations for RANSAC\n",
        "    Output:\n",
        "    - estimated homography H\n",
        "    - inliers\n",
        "    \"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "def stitch_images(img1, img2):\n",
        "    \"\"\"\n",
        "    Stitch two images at a time.\n",
        "    Inputs:\n",
        "    - img1, img2 with shape [H, W, 3].\n",
        "    Output:\n",
        "    - stitched_image with shape [H, W, 3].\n",
        "    \"\"\"\n",
        "    #1. Detect keypoints\n",
        "    # SIFT feature detection and matching\n",
        "    # You can use sift.detectAndCompute() from cv2\n",
        "\n",
        "\n",
        "    #2. Match keypoints\n",
        "    # hint: You will need to create descriptors. One way to do that is to use\n",
        "    # sift. You can use SIFT from opencv (cv2) for that\n",
        "    # or you can use your preferred method of creating descriptors\n",
        "    # KNN can be useful to match points, use e.g., cv2.FlannBasedMatcher()\n",
        "    # You can further filter out some matches by using a threshold.\n",
        "\n",
        "\n",
        "\n",
        "    #3. Estimate homography with matched keypoints (using RANSAC)\n",
        "    # Call `find_homography_ransac()`\n",
        "\n",
        "\n",
        "\n",
        "    #4. Combine images\n",
        "    # Here we assume all the images are ordered, from left to right: \"1.jpg,\n",
        "    # 2.jpg, 3.jpg...\". You can use `cv2.warpPerspective()` to warp images.\n",
        "    # Be careful: do we use forward warping or inverse warping?\n",
        "    # You probably want to crop out some dark part. Call crop_dark() above.\n"
      ],
      "metadata": {
        "id": "OBO9BLc_-EWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first try two images.\n",
        "import glob\n",
        "case_name = \"hill\"\n",
        "imgs_path = \"/content/\" + case_name\n",
        "img_list = sorted(glob.glob(os.path.join(imgs_path, \"*.JPG\"))) + sorted(glob.glob(os.path.join(imgs_path, \"*.jpg\")))\n",
        "print(img_list)\n",
        "\n",
        "\n",
        "# Try first two images\n",
        "img1 = cv2.imread(img_list[1])\n",
        "img2 = cv2.imread(img_list[2])\n",
        "\n",
        "stitched_12 = stitch_images(img1, img2)\n",
        "\n",
        "# display stitched image\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(stitched_12)"
      ],
      "metadata": {
        "id": "mxEkasUsPZOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's stitch all three images in `hill`.\n",
        "# Hint: you can stitch the last two images `2.JPG` and `3.JPG`\n",
        "# and then stitch `1.JPG` with stitched `2.JPG` + `3.JPG`.\n"
      ],
      "metadata": {
        "id": "TE5R2wlV7vEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now stitch images for `tv`.\n",
        "# Try some different hyperparameters,\n",
        "# such as thresh in keypoint match, or error thresh in RANSAC."
      ],
      "metadata": {
        "id": "MrmmRz0eUbIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write-up (15 pt)\n",
        "*  Describe how to remove incorrect matches with RANSAC\n",
        "*  Display the best homography H after RANSAC (We recommend normalizing `H` by `H = H / H[2, 2]`)\n",
        "*  Display the blended images"
      ],
      "metadata": {
        "id": "gCSoLSQON2yj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajIkEAnRTfpI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}